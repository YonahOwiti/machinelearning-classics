{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow tutorial by Rowel Atienza\n",
    "\n",
    "> https://towardsdatascience.com/lstm-by-example-using-tensorflow-feb0c1968537\n",
    "\n",
    "Our objective will be to predict next word given a previous window size. The text is as following\n",
    "\n",
    "> _long ago , the mice had a general council to consider what measures they could take to outwit their common enemy , the cat . some said this , and some said that but at last a young mouse got up and said he had a proposal to make , which he thought would meet the case . you will all agree , said he , that our chief danger consists in the sly and treacherous manner in which the enemy approaches us . now , if we could receive some signal of her approach , we could easily escape from her . i venture , therefore , to propose that a small bell be procured , and attached by a ribbon round the neck of the cat . by this means we should always know when she was about , and could easily retire while she was in the neighbourhood . this proposal met with general applause , until an old mouse got up and said that is all very well , but who is to bell the cat ? the mice looked at one another and nobody spoke . then the old mouse said it is easy to propose impossible remedies ._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "import numpy as np \n",
    "import time \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn \n",
    "\n",
    "start_time= time.time() \n",
    "def elapsed(sec):\n",
    "    if sec<60:\n",
    "        return str(sec) + ' sec'\n",
    "    elif sec<(60*60):\n",
    "        return str(sec/60) + \" min\"\n",
    "    else:\n",
    "        return str(sec/(60*60)) + \" hr\"\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = 'long ago , the mice had a general council to consider what measures they could take to outwit their common enemy , the cat . some said this , and some said that but at last a young mouse got up and said he had a proposal to make , which he thought would meet the case . you will all agree , said he , that our chief danger consists in the sly and treacherous manner in which the enemy approaches us . now , if we could receive some signal of her approach , we could easily escape from her . i venture , therefore , to propose that a small bell be procured , and attached by a ribbon round the neck of the cat . by this means we should always know when she was about , and could easily retire while she was in the neighbourhood . this proposal met with general applause , until an old mouse got up and said that is all very well , but who is to bell the cat ? the mice looked at one another and nobody spoke . then the old mouse said it is easy to propose impossible remedies .'\n",
    "\n",
    "def build_dataset(words):\n",
    "    count= collections.Counter(words).most_common()\n",
    "    dictionary= dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word]= len(dictionary)\n",
    "    reverse_dictionary= dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('know', 87), ('of', 28), ('.', 2), ('thought', 51), ('could', 7)]\n"
     ]
    }
   ],
   "source": [
    "words = text.split()\n",
    "word2idx, idx2word = build_dataset(words)\n",
    "output = random.sample(list(word2idx.items()),k=5)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "vocab_size= len(words)\n",
    "n_input=3 \n",
    "\n",
    "learning_rate=0.001\n",
    "training_iters=50000\n",
    "display_step= 1000\n",
    "n_input=3 \n",
    "\n",
    "#number of units in RNN cell\n",
    "n_hidden= 512\n",
    "\n",
    "#RNN output node weights and biases\n",
    "weights={\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, vocab_size]))\n",
    "}\n",
    "biases= {\n",
    "    'out': tf.Variable(tf.random_normal([vocab_size]))    \n",
    "}\n",
    "\n",
    "#tf Graph input\n",
    "x = tf.placeholder(tf.float32, shape=(None, n_input,1 ), name='x')\n",
    "y = tf.placeholder(tf.float32, shape=(None, vocab_size), name='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cell():\n",
    "    #1-layer LSTM with n_hidden units.\n",
    "    return rnn.core_rnn_cell.BasicLSTMCell(n_hidden)\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "    \n",
    "    #reshape to [1, n_input]\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "    \n",
    "    #Generate a n_input-element sequence of inputs\n",
    "    #(eg. [had] [a] [general] ->  [20] [6] [33])\n",
    "    x = tf.split(x, n_input, 1)\n",
    "    \n",
    "\n",
    "    rnn_cell=cell()\n",
    "    #generate prediction \n",
    "    lstm_cell= rnn.MultiRNNCell([rnn_cell])\n",
    "    outputs, states= rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "    \n",
    "    #there are n_input outputs but\n",
    "    #we only want the last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = RNN(x, weights, biases)\n",
    "\n",
    "# Loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Model evaluation\n",
    "correct_pred= tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "#Initializing the variables\n",
    "init= tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=1000, Average Loss=5.836279, Average Accuracy=5.50\n",
      "['to', 'bell', 'the'] - [cat] vs [cat]\n",
      "Iter=2000, Average Loss=3.794937, Average Accuracy=12.10\n",
      "['until', 'an', 'old'] - [mouse] vs [was]\n",
      "Iter=3000, Average Loss=3.202694, Average Accuracy=20.40\n",
      "['she', 'was', 'about'] - [,] vs [,]\n",
      "Iter=4000, Average Loss=2.577922, Average Accuracy=34.10\n",
      "['by', 'a', 'ribbon'] - [round] vs [round]\n",
      "Iter=5000, Average Loss=1.994354, Average Accuracy=46.50\n",
      "['venture', ',', 'therefore'] - [,] vs [we]\n",
      "Iter=6000, Average Loss=2.185716, Average Accuracy=47.90\n",
      "['approach', ',', 'we'] - [could] vs [could]\n",
      "Iter=7000, Average Loss=1.796441, Average Accuracy=57.80\n",
      "['which', 'the', 'enemy'] - [approaches] vs [approaches]\n",
      "Iter=8000, Average Loss=1.755942, Average Accuracy=57.20\n",
      "['case', '.', 'you'] - [will] vs [will]\n",
      "Iter=9000, Average Loss=1.461480, Average Accuracy=63.70\n",
      "['got', 'up', 'and'] - [said] vs [said]\n",
      "Iter=10000, Average Loss=1.233967, Average Accuracy=68.30\n",
      "['enemy', ',', 'the'] - [cat] vs [cat]\n",
      "Iter=11000, Average Loss=1.383828, Average Accuracy=65.20\n",
      "['council', 'to', 'consider'] - [what] vs [their]\n",
      "Iter=12000, Average Loss=1.088263, Average Accuracy=74.10\n",
      "['it', 'is', 'easy'] - [to] vs [to]\n",
      "Iter=13000, Average Loss=1.190254, Average Accuracy=71.40\n",
      "['at', 'one', 'another'] - [and] vs [and]\n",
      "Iter=14000, Average Loss=1.286524, Average Accuracy=68.60\n",
      "['all', 'very', 'well'] - [,] vs [,]\n",
      "Iter=15000, Average Loss=1.147132, Average Accuracy=71.80\n",
      "['general', 'applause', ','] - [until] vs [until]\n",
      "Iter=16000, Average Loss=0.857085, Average Accuracy=79.10\n",
      "['this', 'means', 'we'] - [should] vs [should]\n",
      "Iter=17000, Average Loss=1.063220, Average Accuracy=74.30\n",
      "['to', 'propose', 'that'] - [a] vs [a]\n",
      "Iter=18000, Average Loss=0.986837, Average Accuracy=77.30\n",
      "['escape', 'from', 'her'] - [.] vs [.]\n",
      "Iter=19000, Average Loss=0.857341, Average Accuracy=79.60\n",
      "['which', 'the', 'enemy'] - [approaches] vs [approaches]\n",
      "Iter=20000, Average Loss=0.973192, Average Accuracy=76.00\n",
      "['said', 'he', ','] - [that] vs [and]\n",
      "Iter=21000, Average Loss=0.855626, Average Accuracy=79.60\n",
      "['a', 'proposal', 'to'] - [make] vs [make]\n",
      "Iter=22000, Average Loss=0.822108, Average Accuracy=79.80\n",
      "['up', 'and', 'said'] - [he] vs [that]\n",
      "Iter=23000, Average Loss=0.826313, Average Accuracy=81.20\n",
      "[',', 'the', 'cat'] - [.] vs [.]\n",
      "Iter=24000, Average Loss=0.750361, Average Accuracy=82.20\n",
      "['easy', 'to', 'propose'] - [impossible] vs [impossible]\n",
      "Iter=25000, Average Loss=0.791106, Average Accuracy=81.00\n",
      "['the', 'cat', '?'] - [the] vs [the]\n",
      "Iter=26000, Average Loss=0.722360, Average Accuracy=83.10\n",
      "['all', 'very', 'well'] - [,] vs [general]\n",
      "Iter=27000, Average Loss=0.822464, Average Accuracy=78.80\n",
      "['with', 'general', 'applause'] - [,] vs [to]\n",
      "Iter=28000, Average Loss=0.620092, Average Accuracy=83.40\n",
      "['when', 'she', 'was'] - [about] vs [in]\n",
      "Iter=29000, Average Loss=0.602254, Average Accuracy=85.40\n",
      "['procured', ',', 'and'] - [attached] vs [could]\n",
      "Iter=30000, Average Loss=0.616745, Average Accuracy=84.80\n",
      "['venture', ',', 'therefore'] - [,] vs [we]\n",
      "Iter=31000, Average Loss=0.695887, Average Accuracy=83.70\n",
      "['enemy', 'approaches', 'us'] - [.] vs [.]\n",
      "Iter=32000, Average Loss=0.573353, Average Accuracy=86.50\n",
      "['our', 'chief', 'danger'] - [consists] vs [in]\n",
      "Iter=33000, Average Loss=0.500480, Average Accuracy=88.30\n",
      "['and', 'said', 'he'] - [had] vs [had]\n",
      "Iter=34000, Average Loss=0.600032, Average Accuracy=84.70\n",
      "['.', 'some', 'said'] - [this] vs [this]\n",
      "Iter=35000, Average Loss=0.526564, Average Accuracy=85.50\n",
      "['had', 'a', 'general'] - [council] vs [to]\n",
      "Iter=36000, Average Loss=0.571253, Average Accuracy=86.60\n",
      "['is', 'to', 'bell'] - [the] vs [the]\n",
      "Iter=37000, Average Loss=0.453261, Average Accuracy=88.40\n",
      "['proposal', 'met', 'with'] - [general] vs [and]\n",
      "Iter=38000, Average Loss=0.685628, Average Accuracy=83.40\n",
      "['the', 'neighbourhood', '.'] - [this] vs [this]\n",
      "Iter=39000, Average Loss=0.496598, Average Accuracy=87.20\n",
      "['and', 'could', 'easily'] - [retire] vs [retire]\n",
      "Iter=40000, Average Loss=0.539212, Average Accuracy=85.70\n",
      "['by', 'a', 'ribbon'] - [round] vs [round]\n",
      "Iter=41000, Average Loss=0.567405, Average Accuracy=86.60\n",
      "['of', 'her', 'approach'] - [,] vs [,]\n",
      "Iter=42000, Average Loss=0.518946, Average Accuracy=87.30\n",
      "['enemy', 'approaches', 'us'] - [.] vs [.]\n",
      "Iter=43000, Average Loss=0.419101, Average Accuracy=89.80\n",
      "['he', ',', 'that'] - [our] vs [our]\n",
      "Iter=44000, Average Loss=0.410329, Average Accuracy=88.90\n",
      "['thought', 'would', 'meet'] - [the] vs [the]\n",
      "Iter=45000, Average Loss=0.457899, Average Accuracy=89.20\n",
      "['had', 'a', 'proposal'] - [to] vs [council]\n",
      "Iter=46000, Average Loss=0.457907, Average Accuracy=89.00\n",
      "[',', 'and', 'some'] - [said] vs [said]\n",
      "Iter=47000, Average Loss=0.515911, Average Accuracy=87.50\n",
      "['general', 'council', 'to'] - [consider] vs [consider]\n",
      "Iter=48000, Average Loss=0.339134, Average Accuracy=90.90\n",
      "['at', 'one', 'another'] - [and] vs [and]\n",
      "Iter=49000, Average Loss=0.415857, Average Accuracy=88.90\n",
      "['well', ',', 'but'] - [who] vs [who]\n",
      "Iter=50000, Average Loss=0.401535, Average Accuracy=89.20\n",
      "['met', 'with', 'general'] - [applause] vs [applause]\n",
      "optimization finished\n",
      "Elapsed time: 7.56718153556188 min\n"
     ]
    }
   ],
   "source": [
    "#Launch the graph\n",
    "with tf.Session() as session:\n",
    "\tsession.run(init)\n",
    "\tstep=0 \n",
    "\toffset= random.randint(0, n_input)\n",
    "\tend_offset= n_input+1 \n",
    "\tacc_total=0 \n",
    "\tloss_total=0 \n",
    "  \n",
    "\twhile (step < training_iters):\n",
    "\t\t#Generate a minibatch. Add some randomness\n",
    "\t\tif offset> (len(words)-end_offset):\n",
    "\t\t    offset=random.randint(0,n_input+1)\n",
    "\n",
    "\n",
    "\n",
    "\t\tsymbols_in_keys= [[word2idx[words[i]]] for i in range(offset, offset+n_input)]   \n",
    "\t\tsymbols_in_keys= np.reshape( np.array(symbols_in_keys), (-1, n_input,1) )\n",
    "\n",
    "\t\tsymbols_out_onehot= np.zeros((vocab_size), dtype=np.float32)\n",
    "\t\tsymbols_out_onehot[word2idx[words[offset+n_input]]]=1.0\n",
    "\t\tsymbols_out_onehot=np.reshape(symbols_out_onehot,[1,-1])\n",
    "\n",
    "\t\t_, acc, loss, one_hotpred= session.run(\n",
    "\t\t    [optimizer, accuracy, cost, pred],\n",
    "\t\t    feed_dict={x: symbols_in_keys, y:symbols_out_onehot}            \n",
    "\t\t)    \n",
    "\t\tloss_total += loss \n",
    "\t\tacc_total  += acc \n",
    "\n",
    "\t\tif (step+1) % display_step==0:\n",
    "\t\t\tmsg= 'Iter=' + str(step+1) \n",
    "\t\t\tmsg+=', Average Loss=' + \"{:.6f}\".format(loss_total/display_step) \n",
    "\t\t\tmsg+= ', Average Accuracy=' + \"{:.2f}\".format(100*acc_total/display_step)\n",
    "\t\t\tprint(msg)\n",
    "\t\t\tacc_total=0\n",
    "\t\t\tloss_total=0\n",
    "\t\t\tsymbols_in= [words[i] for i in range(offset, offset + n_input)]\n",
    "\t\t\tsymbols_out= words[offset + n_input]\n",
    "\t\t\tsymbols_out_pred= idx2word[int(tf.argmax(one_hotpred, 1).eval())]\n",
    "\n",
    "\t\t\tprint(\"%s - [%s] vs [%s]\" % (symbols_in, symbols_out, symbols_out_pred))\n",
    "\n",
    "\t\tstep+=1\n",
    "\t\toffset +=(n_input+1)\n",
    "\t\t\t  \n",
    "print('optimization finished')\n",
    "print('Elapsed time:', elapsed(time.time()-start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
